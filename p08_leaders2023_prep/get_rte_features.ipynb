{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aac29d0e",
   "metadata": {},
   "source": [
    "# RTE. Извлечение фичей из текстовых описаний"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576cefb",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Подготовка к работе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4o9P2TYXpVIR",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# 4COLAB: подключаем гугл-диск\n",
    "from google.colab import drive\n",
    "drive.mount ('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hgyPpGxQrs3i",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13707 sha256=f90670ab207fed2628e126e5ec093bcee531c0c7eac6f59394176141c4efdbc3\n",
      "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.2\n"
     ]
    }
   ],
   "source": [
    "# 4COLAB: установим отсутствующие модули\n",
    "!pip install pymorphy2\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1bb7207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем необходимые модули\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import gensim\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce8fb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# отключаем предупреждения\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "G2kKM5VvxPNI",
   "metadata": {},
   "outputs": [],
   "source": [
    "# настроим вывод датафреймов на экран\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88f8125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# скачаем стоп-слова из nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0Vt6dx0uy5_w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# размеры векторов\n",
    "W2V_SIZE = 300\n",
    "BERT_SIZE = 312\n",
    "\n",
    "# максимальное количество токенов BERT\n",
    "BERT_MAX_LENGTH = 312  # максимум 2048, но долго\n",
    "\n",
    "# путь до рабочей папки\n",
    "OZON_PATH = '/content/drive/MyDrive/Colab Notebooks/Ozon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "J2U71AJB6elg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает разницу во времени в виде строки '(**m **s)' или '(<1s)'\n",
    "def get_time(t1, t2):\n",
    "    dt = t2 - t1\n",
    "    if int(dt):\n",
    "        return '(' + (f'{dt//60:.0f}m ' if dt//60 else '') + f'{dt%60:.0f}s)'\n",
    "    return '(<1s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "m52L0TBc-vcz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция запуска таймера\n",
    "def timer_start(msg):\n",
    "    print(msg, end='')\n",
    "    return time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af1uCOgU-w7r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция остановка таймера\n",
    "def timer_stop(start_time):\n",
    "    print(f'Готово! {get_time(start_time, time.time())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T65CmEZVwcPv",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Загрузка датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae484798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.25 s, sys: 2.68 s, total: 7.93 s\n",
      "Wall time: 27.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# загрузим данные в датафрейм `df`\n",
    "df = pd.read_parquet(OZON_PATH + 'datasets/data_hakaton_2_5.parquet')\n",
    "\n",
    "# переименуем столбцы и расположим в правильном порядке\n",
    "df.columns = ['txt1', 'imgv1', 'imgv2', 'txt2', 'same']\n",
    "df = df[['txt1', 'txt2', 'imgv1', 'imgv2', 'same']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "MNJFP_BAuJxb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9zG2v_zzuPbl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # возьмем часть от датафрейма\n",
    "# fr = 0.001\n",
    "\n",
    "# # соблюдаем изначальную нумерацию строк и баланс классов (с индекса 31088 начинается класс 0)\n",
    "# df = pd.concat([df0[:int(31088*fr)], df0[31088:31088+int(31088*fr)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1xURXgDftMZ_",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62176, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>txt1</th>\n",
       "      <th>txt2</th>\n",
       "      <th>imgv1</th>\n",
       "      <th>imgv2</th>\n",
       "      <th>same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Зарядный кабель Borofone BX1 Lightning для айфон, 1м Зарядный кабель Borofone BX1 подходит для зарядки всех гаджетов и аксессуаров с разъемом Lightning. Поддерживает быструю зарядку. Подходит для ...</td>\n",
       "      <td>кабель проникновения производительность Borofone Lightning для айфон, 1м Зарядный кабель техника Borofone BX1 найти.v подходит Силикон гаджетов и аксессуаров с разъемом Lightning. Поддерживает быс...</td>\n",
       "      <td>[0.071516864, 1.097235, 0.1598482, 0.0, 1.2093426, 0.029174754, 1.1688348, 2.4246013, 0.0, 0.009662722, 0.0, 2.7528465, 0.08303496, 0.0026721905, 0.11616977, 0.17895052, 0.0096440585, 0.7796725, 0...</td>\n",
       "      <td>[0.0, 1.0745188, 0.0069836862, 0.0, 1.5283647, 0.0, 3.236915, 0.9076446, 0.0, 0.044373196, 0.5687515, 3.0410533, 0.0083197225, 0.7042634, 0.74028045, 0.2894601, 0.009714443, 1.33844, 0.0072414633,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                      txt1  \\\n",
       "0  Зарядный кабель Borofone BX1 Lightning для айфон, 1м Зарядный кабель Borofone BX1 подходит для зарядки всех гаджетов и аксессуаров с разъемом Lightning. Поддерживает быструю зарядку. Подходит для ...   \n",
       "\n",
       "                                                                                                                                                                                                      txt2  \\\n",
       "0  кабель проникновения производительность Borofone Lightning для айфон, 1м Зарядный кабель техника Borofone BX1 найти.v подходит Силикон гаджетов и аксессуаров с разъемом Lightning. Поддерживает быс...   \n",
       "\n",
       "                                                                                                                                                                                                     imgv1  \\\n",
       "0  [0.071516864, 1.097235, 0.1598482, 0.0, 1.2093426, 0.029174754, 1.1688348, 2.4246013, 0.0, 0.009662722, 0.0, 2.7528465, 0.08303496, 0.0026721905, 0.11616977, 0.17895052, 0.0096440585, 0.7796725, 0...   \n",
       "\n",
       "                                                                                                                                                                                                     imgv2  \\\n",
       "0  [0.0, 1.0745188, 0.0069836862, 0.0, 1.5283647, 0.0, 3.236915, 0.9076446, 0.0, 0.044373196, 0.5687515, 3.0410533, 0.0083197225, 0.7042634, 0.74028045, 0.2894601, 0.009714443, 1.33844, 0.0072414633,...   \n",
       "\n",
       "   same  \n",
       "0     1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out\n",
    "print(df.shape)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z8cr_J8QQqns",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Обработка текстовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rot8iV0BIWvz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает очищенный текст (только русские, англ. буквы и одиночные пробелы)\n",
    "def clean(text):\n",
    "    text = re.sub(r'\\</?[^\\>]+\\>', ' ', text)                               # html-теги\n",
    "    text = re.sub(r'''(?:http[s]?://|www\\.)(?:(?!\"|'|\\s).)+''', ' ', text)  # веб-ссылки\n",
    "    text = re.sub(r'&[#\\w]+;', ' ', text)                                   # спец-символы html\n",
    "    text = text.lower()\n",
    "    text = text.replace('ё', 'е')\n",
    "    text = re.sub(r'[^a-zа-я]', ' ', text)    # все символы, кроме русских и англ. букв\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', ' ', text)  # слова в 1-2 символа\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)       # более одного пробела\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d9d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает словарь лемм для всех слов в датафрейме\n",
    "def get_lemms(txt_series1, txt_series2):\n",
    "    morph = MorphAnalyzer()\n",
    "    # стоп-слова\n",
    "    stopwords = nltk_stopwords.words('russian') + nltk_stopwords.words('english')\n",
    "    # множество всех слов в обоих столбцах, за исключением стоп-слов\n",
    "    words = set(' '.join(set(txt_series1).union(set(txt_series2))).split()) - set(stopwords)\n",
    "    # возвращаем словарь {слово: лемма}\n",
    "    return {word: morph.normal_forms(word)[0].replace('ё', 'е') for word in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bb161e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция лемматизации текста\n",
    "def lemmatize(text):\n",
    "    return ' '.join([lemms[word] for word in text.split() if word in lemms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6GlMLkIbQibv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь соответствия POS-тегов pymorphy2 и word2vec\n",
    "POS_TAGS = {'ADJF': 'ADJ', 'ADJS': 'ADJ', 'ADVB': 'ADV', 'COMP': 'ADV', 'PRED': 'ADV', \n",
    "            'INTJ': 'INTJ', 'NOUN': 'NOUN', 'NPRO': 'NOUN', 'NUMR': 'NUM', \n",
    "            'GRND': 'VERB', 'INFN': 'VERB', 'PRTF': 'VERB', 'PRTS': 'VERB', 'VERB': 'VERB', \n",
    "            'CONJ': 'X', 'PRCL': 'X', 'PREP': 'X'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "NGIT_3LbQiTg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает POS-тег\n",
    "def get_pos_tag(word, morph):\n",
    "    pos_pymorphy2 = morph.parse(word)[0].tag.POS\n",
    "    if pos_pymorphy2 in POS_TAGS:\n",
    "        return POS_TAGS[pos_pymorphy2]\n",
    "    return 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "QAXfmNPWQiLF",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает словарь pos-тегов для всех лемм из словаря lemms\n",
    "def get_pos_tags(lemms_dct):\n",
    "    morph = MorphAnalyzer()\n",
    "    return {lemm: get_pos_tag(lemm, morph) for lemm in set(lemms_dct.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9U0p378QSAwl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция POS-тегирования слов (лемм) в тексте\n",
    "def pos_tagging(text):\n",
    "    return ' '.join([word + '_' + pos_tags[word] for word in text.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t7aUsLhKQTq7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "#### Обработка текстовых данных\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7zFFyH6QdcH",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очистка текстовых данных... Готово! (19s)\n",
      "Подготовка множества всех слов и словаря лемм для каждого слова... Готово! (19s)\n",
      "Лемматизация... Готово! (4s)\n",
      "Подготовка словаря POS-тегов для каждой леммы... Готово! (6s)\n",
      "POS-тегирование текстов... Готово! (3s)\n"
     ]
    }
   ],
   "source": [
    "start = timer_start('Очистка текстовых данных... ')\n",
    "for col in ['txt1', 'txt2']:\n",
    "    df[col+'_lemm'] = df[col].apply(clean)\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Подготовка множества всех слов и словаря лемм для каждого слова... ')\n",
    "lemms = get_lemms(df['txt1_lemm'], df['txt2_lemm'])\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Лемматизация... ')\n",
    "for col in ['txt1_lemm', 'txt2_lemm']:\n",
    "    df[col] = df[col].apply(lambda x: lemmatize(x))\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Подготовка словаря POS-тегов для каждой леммы... ')\n",
    "pos_tags = get_pos_tags(lemms)\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('POS-тегирование текстов... ')\n",
    "for col in ['txt1_lemm', 'txt2_lemm']:\n",
    "    df[col[:5]+'pos'] = df[col].apply(pos_tagging)\n",
    "timer_stop(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Iq5_al90P35p",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Вычисление признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Zfj6ewHr0Ng2",
   "metadata": {},
   "source": [
    "---\n",
    "#### Функции: BERT-фичи\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bf9dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция вычисления BERT эмбеддингов\n",
    "def get_bert_embeddings_tensor(text):\n",
    "    tk = tokenizer(text, max_length=BERT_MAX_LENGTH, truncation=True, padding='max_length', \n",
    "                   add_special_tokens=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in tk.items()})\n",
    "    embeddings = model_output.last_hidden_state\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36xqUkGFAPGb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает массив векторов эмбеддингов слов из BERT-тензора (склеивает токены частей слов)\n",
    "def get_words_embeddings(text, embeddings):\n",
    "    # список токенов\n",
    "    tokens = ['[CLS]'] + tokenizer.tokenize(text)[:BERT_MAX_LENGTH-2] + ['[SEP]']\n",
    "\n",
    "    # # словарь соответствий токенов и эмбеддингов (раскомментировать для отладки)\n",
    "    # print('tokens:', tokens)\n",
    "    # print('len(tokens) =', len(tokens))\n",
    "    # token_embeddings = {}\n",
    "    # for i in range(len(tokens)):\n",
    "    #     token_embeddings[tokens[i]] = embeddings[i]\n",
    "    #     print(f'i={i:<3}  {tokens[i]:14s}  {token_embeddings[tokens[i]][:3]}')\n",
    "    # print()\n",
    "    # k = 0  # для распечатки word_embeddings\n",
    "\n",
    "    # склеиваем токены частей слов в целые слова и получаем соответствующие им эмбеддинги (среднее всех частей слова)\n",
    "    words_embeddings = []\n",
    "    i = 1                       # исключаем нулевой индекс ([CLS])\n",
    "    while i < len(tokens) - 1:  # исключаем последний индекс ([SEP])\n",
    "        if tokens[i][:2] == '##':\n",
    "            i += 1\n",
    "        else:\n",
    "            j = i + 1\n",
    "            while j < len(tokens) - 1 and tokens[j][:2] == '##':\n",
    "                j += 1\n",
    "            words_embeddings.append(np.mean(embeddings[i:j], axis=0))\n",
    "            # # распечатка эмбеддингов склеенных слов (раскомментировать для отладки)\n",
    "            # print(f\"k={k:<3} \", \n",
    "            #       f\"{''.join((''.join(tokens[i:j])).split('##')):14s} \", \n",
    "            #       f\"{np.mean(embeddings[i:j], axis=0)[:3]}\")\n",
    "            # k += 1\n",
    "            i = j\n",
    "    \n",
    "    # возвращаем список векторов эмбеддингов слов\n",
    "    return words_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "HbBq5uBVSJeW",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция вычисляет EMDV для BERT-векторов слов\n",
    "def get_bert_emdv(words_vecs1, words_vecs2):\n",
    "    vec_s1 = get_semantic_info(words_vecs1, BERT_SIZE)\n",
    "    vec_s2 = get_semantic_info(words_vecs2, BERT_SIZE)\n",
    "    return abs(vec_s2 - vec_s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "iuChb8KG_4P7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает кортеж из BERT-фичей\n",
    "def bert_features(text1, text2):\n",
    "    # тензоры BERT\n",
    "    embeddings1 = get_bert_embeddings_tensor(text1)\n",
    "    embeddings2 = get_bert_embeddings_tensor(text2)\n",
    "\n",
    "    # эмбеддинги текстов\n",
    "    bert1 = embeddings1[0]\n",
    "    bert2 = embeddings2[0]\n",
    "\n",
    "    # косинусное сходство между векторами bert1 и bert2\n",
    "    bert_cos = cosine_similarity(bert1.reshape(1, -1), bert2.reshape(1, -1))[0][0]\n",
    "\n",
    "    # списки эмбеддингов слов\n",
    "    words_vecs1 = get_words_embeddings(text1, embeddings1)\n",
    "    words_vecs2 = get_words_embeddings(text2, embeddings2)\n",
    "\n",
    "    # эмбеддинги текстов, как поэлементные суммы эмбеддингов слов\n",
    "    wbert1 = np.sum(np.array(words_vecs1), axis=0) if words_vecs1 else np.zeros(BERT_SIZE, dtype='float32')\n",
    "    wbert2 = np.sum(np.array(words_vecs2), axis=0) if words_vecs2 else np.zeros(BERT_SIZE, dtype='float32')\n",
    "\n",
    "    # косинусное сходство между векторами wbert1 и wbert2\n",
    "    bert_sts = cosine_similarity(wbert1.reshape(1, -1), wbert2.reshape(1, -1))[0][0]\n",
    "\n",
    "    # # EMDV для BERT-векторов слов  (закомментированы - не показали хорошее качество)\n",
    "    # bert_emdv = get_bert_emdv(words_vecs1, words_vecs2)\n",
    "    # bert_emdv_mean = bert_emdv.mean()\n",
    "\n",
    "    return (bert1.tolist(), \n",
    "            bert2.tolist(), \n",
    "            bert_cos, \n",
    "            wbert1.tolist(), \n",
    "            wbert2.tolist(), \n",
    "            bert_sts)  # , bert_emdv.tolist(), bert_emdv_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w7Sg6fhZzANU",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Функции: Поэлементный вектор Манхэттенских расстояний (EMDV)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "gQOcY0LWlA86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает список w2v-векторов слов\n",
    "def get_w2v_vectors(text_pos):\n",
    "    return [w2v_model[word] for word in text_pos.split() if word in w2v_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "NbJAuvtAgzYB",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращает семантический вектор текста (векторы слов добавляются в него поэлементно, по порогу)\n",
    "def get_semantic_info(words_vecs, vec_size):\n",
    "    # если пустой список векторов, возвращаем массив нулей размером vec_size\n",
    "    if not words_vecs:\n",
    "        return np.zeros(vec_size, dtype='float32')\n",
    "    vec_s = words_vecs[0].copy()\n",
    "    for i in range(1, len(words_vecs)):\n",
    "        x = words_vecs[i]\n",
    "        alpha = x.mean() + x.std()\n",
    "        for k in range(vec_size):\n",
    "            if abs(vec_s[k] - x[k]) >= alpha:\n",
    "                vec_s[k] += x[k]\n",
    "    return vec_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "lvDCAPFGssW0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция вычисляет EMDV для w2v-векторов слов\n",
    "def get_emdv(text1_pos, text2_pos):\n",
    "    words_vecs1 = get_w2v_vectors(text1_pos)\n",
    "    words_vecs2 = get_w2v_vectors(text2_pos)\n",
    "    vec_s1 = get_semantic_info(words_vecs1, W2V_SIZE)\n",
    "    vec_s2 = get_semantic_info(words_vecs2, W2V_SIZE)\n",
    "    return abs(vec_s2 - vec_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cHdxilZQ0ADH",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Функции: Сходство по Жаккару\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fLQtg829q8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция вычисляет сходство по Жаккару\n",
    "def jaccar(text1, text2):\n",
    "    t1 = set(text1.split())\n",
    "    t2 = set(text2.split())\n",
    "    jac = len(t1 & t2) / len(t1 | t2)\n",
    "    return jac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lF8bs2y60ApP",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Функции: Сходство на основе мешка слов\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "meN6EiOOVyVD",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция рассчитывает косинусное сходство между двумя векторами из мешка слов\n",
    "def bow_similarity(text1, text2):\n",
    "    count_vect = CountVectorizer()\n",
    "    corpus = [text1, text2]\n",
    "    bow = count_vect.fit_transform(corpus)\n",
    "    return cosine_similarity(bow[0], bow[1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Hx9i3H0Dk1VY",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "#### Вычисление признаков\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1Uvnvwd1ejjl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]\n",
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/1.08M [00:00<?, ?B/s]\n",
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.74M [00:00<?, ?B/s]\n",
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]\n",
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]\n",
       "Downloading pytorch_model.bin:   0%|          | 0.00/118M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загрузка предобученной BERT-модели\n",
    "tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n",
    "model = AutoModel.from_pretrained('cointegrated/rubert-tiny2')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "NGGhDcMb6SIl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вычисление BERT-фичей... Готово! (16m 10s)\n",
      "Рассчитываем косинусное сходство между векторами эмбеддингов изображений... Готово! (25s)\n",
      "Загрузка предобученной w2v-модели для расчёта emdv... Готово! (7s)\n",
      "Рассчитываем поэлементный вектор расстояний Манхэттена (EMDV)... Готово! (31m 29s)\n",
      "Рассчитываем среднее значение EMDV... Готово! (<1s)\n",
      "Рассчитываем сходство по Жаккару... Готово! (3s)\n",
      "Рассчитываем сходство на основе мешка слов... Готово! (2m 10s)\n"
     ]
    }
   ],
   "source": [
    "start = timer_start('Вычисление BERT-фичей... ')\n",
    "bert_series = df.apply(lambda row: bert_features(row['txt1_lemm'], row['txt2_lemm']), axis=1)\n",
    "df[['bert1', 'bert2', 'bert_cos', 'wbert1', 'wbert2', 'bert_sts']] = pd.DataFrame(\n",
    "    np.array(list(bert_series)), index=bert_series.index)  # , 'bert_emdv', 'bert_emdv_mean'\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Рассчитываем косинусное сходство между векторами эмбеддингов изображений... ')\n",
    "df['imgv_cos'] = df.apply(\n",
    "    lambda row: cosine_similarity(row['imgv1'].reshape(1, -1), row['imgv2'].reshape(1, -1))[0][0], axis=1)\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Загрузка предобученной w2v-модели для расчёта emdv... ')\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    OZON_PATH + 'rte/ruwikiruscorpora_upos_skipgram_300_2_2019.bin', binary=True)\n",
    "# w2v_model.vector_size  # размер выходного вектора 300\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Рассчитываем поэлементный вектор расстояний Манхэттена (EMDV)... ')\n",
    "df['emdv'] = df.apply(lambda row: get_emdv(row['txt1_pos'], row['txt2_pos']), axis=1)\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Рассчитываем среднее значение EMDV... ')\n",
    "df['emdv_mean'] = df['emdv'].apply(lambda x: x.mean())\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Рассчитываем сходство по Жаккару... ')\n",
    "df['jac'] = df.apply(lambda row: jaccar(row['txt1_lemm'], row['txt2_lemm']), axis=1)\n",
    "timer_stop(start)\n",
    "\n",
    "start = timer_start('Рассчитываем сходство на основе мешка слов... ')\n",
    "df['bow'] = df.apply(lambda row: bow_similarity(row['txt1_lemm'], row['txt2_lemm']), axis=1)\n",
    "timer_stop(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oR4lAaiqb-j3",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "Vxww0FX4cTB0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запись датафрейма, часть вторая, полная версия, со всеми столбцами и фичами, включая векторы картинок и BERT-фичи\n",
    "df.to_parquet(OZON_PATH + 'datasets/ds_2_5_features_all.parquet')\n",
    "\n",
    "# запись датафрейма, всё, кроме эмбеддингов изображений\n",
    "df[['txt1', 'txt2', 'txt1_lemm', 'txt2_lemm', 'txt1_pos', 'txt2_pos', \n",
    "    'imgv_cos', 'emdv', 'emdv_mean', 'jac', 'bow', \n",
    "    'bert1', 'bert2', 'bert_cos', 'wbert1', 'wbert2', 'bert_sts', 'same']].to_parquet(\n",
    "        OZON_PATH + 'datasets/ds_2_5_features_all_except_imgv.parquet')\n",
    "\n",
    "# запись датафрейма только со скалярными фичами изображений и текстов\n",
    "df[['txt1', 'txt2', 'txt1_lemm', 'txt2_lemm', 'txt1_pos', 'txt2_pos', \n",
    "    'imgv_cos', 'emdv_mean', 'jac', 'bow', 'bert_cos', 'bert_sts', 'same']].to_parquet(\n",
    "        OZON_PATH + 'datasets/ds_2_5_features_scalar.parquet')\n",
    "\n",
    "# запись датафрейма, только 5 фичей метода RTE из статьи, для бейзлайна мультимодальной модели\n",
    "df[['emdv', 'emdv_mean', 'jac', 'bow', 'bert_sts']].to_parquet(\n",
    "        OZON_PATH + 'datasets/ds_2_5_rte_five_features_312_tokens.parquet')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1cVeZLqgnOvWw-nVz6VlvNKm5mv66sU6-",
     "timestamp": 1683479657082
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
